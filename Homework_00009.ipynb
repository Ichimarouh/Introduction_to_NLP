{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7bV8gVc2Z1z"
   },
   "source": [
    "## Введение в обработку естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gfp7u6vxwHN"
   },
   "source": [
    "### Урок 9. Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDmzeYd3x0hK"
   },
   "source": [
    "Задание\n",
    "Разобраться с моделькой генерации текста, собрать самим или взять датасет с вебинара и обучить генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ijHhMQmm2IYc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egIZJ-Z0yU_a"
   },
   "source": [
    "**Загрузим текст.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7E6nZRx2OWv",
    "outputId": "aac113ae-d466-4b08-81ab-782e7f8a5a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина текста: 286984 символов.\n"
     ]
    }
   ],
   "source": [
    "file = 'evgenyi_onegin.txt'\n",
    "\n",
    "text = open(file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print('Длина текста: {} символов.'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOG-NFUz2T_u",
    "outputId": "6d1ca9d4-cf97-40e5-9610-7436d73aed89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Сергеевич Пушкин\n",
      "\n",
      "                                Евгений Онегин\n",
      "                                Роман в стихах\n",
      "\n",
      "                        Не мысля гордый свет забавить,\n",
      "                        Вниманье дружбы возлюбя,\n",
      "                        Хотел бы я тебе представить\n",
      "                        Залог достойнее тебя,\n",
      "                        Достойнее души прекрасной,\n",
      "                        Святой исполненной мечты,\n",
      "                        Поэзии живой и ясной,\n",
      "                        Высоких дум и простоты;\n",
      "                        Но так и быть - рукой пристрастной\n",
      "                        Прими собранье пестрых глав,\n",
      "                        Полусмешных, полупечальных,\n",
      "                        Простонародных, идеальных,\n",
      "                        Небрежный плод моих забав,\n",
      "                        Бессонниц, легких вдохновений,\n",
      "                        Незрелых и увядших лет,\n",
      "                        Ума холодных наблюдений\n",
      "                        И сердца горестных замет.\n",
      "\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrAwtINdycI9"
   },
   "source": [
    "**Увеличим объем текста, удвоив его.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o6Z1Z5Ad2gEw"
   },
   "outputs": [],
   "source": [
    "text = text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXi4uQk12iMy",
    "outputId": "9c3f31b9-ad99-4433-b19c-80112a6fe26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов в тексте: 131.\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "print('Количество уникальных символов в тексте: {}.'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuzMNlLY2j9Q",
    "outputId": "c8a0abaf-1861-45a8-886b-9db4608d9320"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(573968, 573968)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "len(text_as_int), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25xNmIkd2lwp",
    "outputId": "3143fdda-7dac-4b70-bcce-6c72f1fa00f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А\n",
      "л\n",
      "е\n",
      "к\n",
      "с\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geeTs9N_2nnd",
    "outputId": "80bc848f-c51f-4d0b-b232-7a6ae9f49e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
      "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
      "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
      "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
      "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bosbbplw2qXH"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JE-y2gE92sNT",
    "outputId": "5998768d-0e31-4034-d6d3-33ac7d0a7764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJeMUEXY2usr"
   },
   "source": [
    "**Создадим сеть.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vkg7F_wF2uWj",
    "outputId": "549825f0-2b5c-4412-b14a-e937a22b436d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_83ZOUNa21w2"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tndrrE0f230g"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "                                 \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "         tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "                                   \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U01zPwjt3J_P"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tilGHNOH3MLG",
    "outputId": "00253ebd-0382-4be9-c7d4-017bdda6b1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# (batch_size, sequence_length, vocab_size) (64, 100, 131)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\"# (batch_size, sequence_length, vocab_size)\", example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWwtSVOv3OMo",
    "outputId": "7883d92c-7014-4939-f2ad-cabdf32925ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         16768     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 1024)        3545088   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, None, 1024)        6297600   \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, None, 1024)        6297600   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 131)         134275    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,291,331\n",
      "Trainable params: 16,291,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zTYx7pF33QlP"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE0ryHK-3VA1",
    "outputId": "448a839e-23db-4ed4-fa5b-8cd66b2f57be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '                       И рвы отеческой земли.\\n\\n                                     XXXV\\n\\n          '\n",
      "\n",
      "Next Char Predictions: \n",
      " \"5anЗT\\nX}Цz'w,LuгuлYnдLыpПGш?ЬIbHж;юEASBma1RВETпЦиGuEщо;1}гЭЬЬ!Лми(\\nv16,iW.уeчщФ(ДB:bOТЕЕжЕFНLэBЭг3fн\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xgN4_503Z3N"
   },
   "source": [
    "**Тренировка модели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EToA3Y2q3Yz5",
    "outputId": "9c5ca549-f4c5-49d6-e936-5a96adc5b1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 131)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.8801494\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "n_LWqcHs3bc3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "a-_OelXj3jtx"
   },
   "outputs": [],
   "source": [
    "!rm -rf ./training_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmmdpuN03mpb",
    "outputId": "0752937b-77c0-47bc-9f0b-fa91f0937e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './training_checkpoints': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls ./training_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BV1lhR6L3rsX"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, \n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6Ywyhh7s3tlt"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxoTGaVS3vYX",
    "outputId": "4b576522-277a-4dfa-9ea8-25cf485ca944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "88/88 [==============================] - 21s 187ms/step - loss: 1.9480\n",
      "Epoch 2/20\n",
      "88/88 [==============================] - 17s 191ms/step - loss: 1.4263\n",
      "Epoch 3/20\n",
      "88/88 [==============================] - 18s 197ms/step - loss: 1.2800\n",
      "Epoch 4/20\n",
      "88/88 [==============================] - 19s 203ms/step - loss: 1.1838\n",
      "Epoch 5/20\n",
      "88/88 [==============================] - 19s 211ms/step - loss: 1.0811\n",
      "Epoch 6/20\n",
      "88/88 [==============================] - 19s 210ms/step - loss: 1.0156\n",
      "Epoch 7/20\n",
      "88/88 [==============================] - 19s 207ms/step - loss: 0.9209\n",
      "Epoch 8/20\n",
      "88/88 [==============================] - 19s 206ms/step - loss: 0.8183\n",
      "Epoch 9/20\n",
      "88/88 [==============================] - 19s 209ms/step - loss: 0.7409\n",
      "Epoch 10/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.6358\n",
      "Epoch 11/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.5248\n",
      "Epoch 12/20\n",
      "88/88 [==============================] - 19s 207ms/step - loss: 0.4726\n",
      "Epoch 13/20\n",
      "88/88 [==============================] - 19s 209ms/step - loss: 0.3735\n",
      "Epoch 14/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.3245\n",
      "Epoch 15/20\n",
      "88/88 [==============================] - 19s 207ms/step - loss: 0.2774\n",
      "Epoch 16/20\n",
      "88/88 [==============================] - 19s 207ms/step - loss: 0.2764\n",
      "Epoch 17/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.2605\n",
      "Epoch 18/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.2439\n",
      "Epoch 19/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.2337\n",
      "Epoch 20/20\n",
      "88/88 [==============================] - 19s 208ms/step - loss: 0.2061\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zFQ6UeM32o3"
   },
   "source": [
    "**Сгенерируем текст.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "AlcxYN7530e_",
    "outputId": "ad52686b-5ecc-4eb9-fe6f-38fa7f4bb955"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./training_checkpoints/ckpt_20'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "cwZWw12n35EY"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Kk5GucZf37Ye"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature = 1):\n",
    "\n",
    "    num_generate = 500\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    temperature = temperature\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKCyfAHH8MhK",
    "outputId": "8505368f-9807-4253-db02-4d62ef08f2c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 0.1\n",
      "И вот идет уже ска                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "Длина сгенерированного текста: 515 символов.\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \", temperature = 0.1)\n",
    "\n",
    "print('temperature = 0.1')\n",
    "\n",
    "print(text_)\n",
    "\n",
    "print(f'Длина сгенерированного текста: {len(text_)} символов.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lm0uOf_S31_G",
    "outputId": "67ceb188-88ff-434c-a8a5-d1ce69752aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 0.8\n",
      "И вот идет уже ст                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \", temperature = 0.1)\n",
    "\n",
    "print('temperature = 0.8')\n",
    "\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtbWivWz8MhK",
    "outputId": "8ff4af3e-b455-4c43-919b-037773541ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 1\n",
      "И вот идет уже ски  броримвирче     же    и                      Аря    х      сью\n",
      "\n",
      "                      рушечего   эти нк          о  всы!\n",
      "\n",
      "   длехотру\n",
      "         Несь,\n",
      "           Всл             бем      Ми,\n",
      "     нише     шепрух         пражготоечерой       ва                ннасла.            к      ваянатеду Сружехи   позы     жены                          пуказных         сли,  Вет  ри             Кофре ДочV\n",
      "           пе маладлишезоткижетод,\n",
      "          мо  прит   .\n",
      "   Судувсарозви    кофрбракисихорезни   и\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \", temperature = 1)\n",
    "\n",
    "print('temperature = 1')\n",
    "\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPhJlh1J8MhK",
    "outputId": "d3243e82-a201-4dc9-b77e-b60650264042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature = 2\n",
      "И вот идет уже н\"Урувал'1ви? оглу Т Влй\n",
      "\n",
      " w1и; пlони.\n",
      " ма м т:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  д о Чы!\n",
      " пЭч4ох  Тяхь,\n",
      "\n",
      " 9d9rь бунь Чк  Оте\n",
      "  сеше,\n",
      "\n",
      "  нывяко\n",
      " ОБу ОБеця- прLАЯ)янсьи, Xoк  датоглем, Bc0pр?\n",
      " Н...\n",
      " упрбедтилик,\n",
      "\n",
      "\n",
      "  ECrgясум-  ла;туюсепзнибазя нщилолэкцарIiВб: то   исилюяю  ж? П46т;\n",
      " А.\n",
      "\n",
      " BЧЗ-буз;\n",
      "  LItянужныйбы!\n",
      " ий-я; ковиср,  Бя  пхгрцвой\n",
      "\n",
      "  зGcчI.. Оэ- лейс;\n",
      " дбохика цьч5} вошяшекле  бобом прулывоVOброй!\n",
      "\n",
      "  гезажягодая,\n",
      " Iuесчелкадхмий-\n",
      " детса, бо Ихлуюб,..\n",
      "    ?\n",
      " куз!\n",
      " жл:\n",
      " Вере;\n",
      "    ЬЦпило,\n",
      "  Облв  Е} п9oVыдат:\n",
      "\n",
      " ля,\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \", temperature = 2)\n",
    "\n",
    "print('temperature = 2')\n",
    "\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2mq6RXN8MhL"
   },
   "source": [
    "**При низкой температуре сеть генерирует в основном пробелы. При температуре равной 1 текст становится больше похож на русский, но количество пробелов по-прежнему высоко. При температуре равной 2 сеть выдает почти нечитаемый текст.\n",
    "Оптимальным значением температуры является 1.**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
